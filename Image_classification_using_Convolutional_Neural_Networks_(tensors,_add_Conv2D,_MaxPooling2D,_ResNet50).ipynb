{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "TPU",
    "celltoolbar": "Create Assignment",
    "colab": {
      "name": "Image classification using Convolutional Neural Networks (tensors,  add Conv2D, MaxPooling2D, ResNet50).ipynb",
      "provenance": [],
      "collapsed_sections": [
        "IRndYGj-NpYd",
        "ic_wzFnprwXI"
      ],
      "private_outputs": true,
      "machine_shape": "hm"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "hP6M3V-YHQg2"
      },
      "source": [
        "from IPython.display import YouTubeVideo\n",
        "YouTubeVideo('MPU2HistivI', width=600, height=400)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ly00NdnJHQg5",
        "toc-hr-collapsed": false
      },
      "source": [
        "# 1. Convolution & Pooling\n",
        "<a id=\"p1\"></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "06h_3Bj3HQg6",
        "toc-hr-collapsed": true
      },
      "source": [
        "## Overview\n",
        "Just like perceptrons and Fully Connected neural networks, CNNs are also inspired by biology - specifically, the receptive fields of the visual cortex.\n",
        "\n",
        "[Hubel and Weisel Nobel Prize 1981](https://www.nobelprize.org/prizes/medicine/1981/summary/)<br>\n",
        "In the brain, the neurons in the visual cortex **specialize** to be receptive to certain shapes, colors, orientations, and other common visual features. \n",
        "\n",
        "Our visual system transforms raw visual input (light from a scene), and sends it to neurons in the brain that evolved to specialize in responding to certain visual stimuli.\n",
        "\n",
        "CNNs (Convolutional Neural Networks) were invented to imitate this approach. A convolution is a mathematical operation that involves passing a filter array with a small receptive field over an image and computing a feature map based on small patches of the image. and Convolutions have a [variety of nice mathematical properties](https://en.wikipedia.org/wiki/Convolution#Properties) - commutativity, associativity, distributivity, and more. Applying a convolution effectively transforms the \"shape\" of the input. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4M5hnthyHQg6"
      },
      "source": [
        "from IPython.display import YouTubeVideo\n",
        "YouTubeVideo('IOHayh06LJ4', width=600, height=400)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Iw3X-_GsHQg8",
        "toc-hr-collapsed": false
      },
      "source": [
        "## Follow Along\n",
        "\n",
        "What are the operations of Convolution and Pooling?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "krm3RupmHQg9"
      },
      "source": [
        "### Convolution\n",
        "\n",
        "A convolution is a transformation of an image to produce a feature map.\n",
        "\n",
        "![](https://lambdaschool-data-science.s3.amazonaws.com/images/Unit4/Sprint2/Module2/Screen+Shot+2020-02-25+at+10.27.17+AM.png)\n",
        "\n",
        "*Image Credits from __Hands on Machine Learning with Sckit-Learn, Keras & TensorFlow__*\n",
        "\n",
        "\n",
        "Helpful Terms:\n",
        "- __Filter__: The weights (parameters) we will apply to our input image.\n",
        "- __Stride__: How the filter moves across the image\n",
        "- __Padding__: Zeros (or other values) around the  the input image border (kind of like a frame of zeros). "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OsAcbKvoeaqU"
      },
      "source": [
        "import imageio\n",
        "import matplotlib.pyplot as plt\n",
        "from skimage import color, io\n",
        "from skimage.exposure import rescale_intensity\n",
        "from sklearn.datasets import load_sample_image\n",
        "\n",
        "austen = io.imread('https://dl.airtable.com/S1InFmIhQBypHBL0BICi_austen.jpg')\n",
        "print(f'RGB image shape: {austen.shape}')\n",
        "austen_grayscale = rescale_intensity(color.rgb2gray(austen))\n",
        "print(f'Grayscale image shape: {austen_grayscale.shape}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KN-ibr_DhyaV"
      },
      "source": [
        "plt.imshow(austen_grayscale, cmap=\"gray\");"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QopB0uo6lNxq"
      },
      "source": [
        "import numpy as np\n",
        "import scipy.ndimage as nd\n",
        "\n",
        "# this a convolutional filter (i.e. a weight matrix) that enhances horizontal features in an image\n",
        "\n",
        "# CHALLENGE: try to convince yourself that you understand how these matrices are able to do what they do  \n",
        "horizontal_edge_convolution = np.array([[ 1, 1, 1, 1],\n",
        "                                        [ 0, 0, 0, 0],\n",
        "                                        [ 0, 0, 0, 0],\n",
        "                                        [-1,-1,-1,-1]])\n",
        "\n",
        "# this a convolutional filter (i.e. a weight matrix) that enhances the vertical features in an image \n",
        "vertical_edge_convolution = np.array([[  1, 0, 0, -1],\n",
        "                                     [   1, 0, 0, -1],\n",
        "                                     [   1, 0, 0, -1],\n",
        "                                     [   1, 0, 0, -1]])\n",
        "                                    \n",
        "\n",
        "# this a convolutional filter (i.e. a weight matrix) that enhances diagonal features in an image \n",
        "diag_edge_convolution = np.array([[  1,-1, 1,-1],\n",
        "                                  [ -1, 1,-1, 1],\n",
        "                                  [  1,-1, 1,-1],\n",
        "                                  [ -1, 1,-1, 1]])\n",
        "\n",
        "# Doc for nd.convolve: https://docs.scipy.org/doc/scipy/reference/generated/scipy.ndimage.convolve.html\n",
        "austen_edges_vert = nd.convolve(austen_grayscale, vertical_edge_convolution)\n",
        "austen_edges_horz = nd.convolve(austen_grayscale, horizontal_edge_convolution)\n",
        "austen_edges_diag = nd.convolve(austen_grayscale, diag_edge_convolution)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-LwEpFW1l-6b"
      },
      "source": [
        "plt.imshow(austen_edges_vert, cmap=\"gray\");"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8TyaToNjHQhH"
      },
      "source": [
        "plt.imshow(austen_edges_horz, cmap=\"gray\");"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "haaOosSQOaue"
      },
      "source": [
        "plt.imshow(austen_edges_diag, cmap=\"gray\");"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "emWasp6AiDXO"
      },
      "source": [
        "### Exercise: apply the vertical and horizontal convolution filters <br>\n",
        "to the `china.jpg` image or any `.jpg` image you want to use!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0lM3qcYz7qag"
      },
      "source": [
        "china = load_sample_image(\"china.jpg\")\n",
        "china.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FGrdhAHikkRV"
      },
      "source": [
        "## YOUR CODE HERE\n",
        "china_grayscale = rescale_intensity(color.rgb2gray(china))\n",
        "print(f'Grayscale image shape: {china_grayscale.shape}')\n",
        "plt.imshow(china_grayscale, cmap=\"gray\");"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import scipy.ndimage as nd\n",
        "\n",
        "# this a convolutional filter (i.e. a weight matrix) that enhances horizontal features in an image\n",
        "\n",
        "# CHALLENGE: try to convince yourself that you understand how these matrices are able to do what they do  \n",
        "horizontal_edge_convolution = np.array([[ 1, 1, 1, 1],\n",
        "                                        [ 0, 0, 0, 0],\n",
        "                                        [ 0, 0, 0, 0],\n",
        "                                        [-1,-1,-1,-1]])\n",
        "\n",
        "# this a convolutional filter (i.e. a weight matrix) that enhances the vertical features in an image \n",
        "vertical_edge_convolution = np.array([[  1, 0, 0, -1],\n",
        "                                     [   1, 0, 0, -1],\n",
        "                                     [   1, 0, 0, -1],\n",
        "                                     [   1, 0, 0, -1]])\n",
        "                                    \n",
        "\n",
        "# this a convolutional filter (i.e. a weight matrix) that enhances diagonal features in an image \n",
        "diag_edge_convolution = np.array([[  1,-1, 1,-1],\n",
        "                                  [ -1, 1,-1, 1],\n",
        "                                  [  1,-1, 1,-1],\n",
        "                                  [ -1, 1,-1, 1]])\n",
        "\n",
        "# Doc for nd.convolve: https://docs.scipy.org/doc/scipy/reference/generated/scipy.ndimage.convolve.html\n",
        "china_edges_vert = nd.convolve(china_grayscale, vertical_edge_convolution)\n",
        "china_edges_horz = nd.convolve(china_grayscale, horizontal_edge_convolution)\n",
        "china_edges_diag = nd.convolve(china_grayscale, diag_edge_convolution)"
      ],
      "metadata": {
        "id": "X6JodaBtdoeV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.imshow(china_edges_diag, cmap=\"gray\");"
      ],
      "metadata": {
        "id": "CIdejUBEd37X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SPhfrit5NpYd"
      },
      "source": [
        "------\n",
        "\n",
        "### Convolutional Neurons \n",
        "\n",
        "In [`scipy.ndimage.convolve`](https://docs.scipy.org/doc/scipy/reference/generated/scipy.ndimage.convolve.html), a one-dimensional convolution calculation is given by<br><br>\n",
        "$$C_i = \\sum_j{W_j X_{i+k-j} }$$ <br>\n",
        "where $C_i$ is the value of the convolution at the $ith$ position. <br>\n",
        "A two-dimensional convolution can be expressed by a similar formula.\n",
        "\n",
        "Don't worry about the various indices: focus on the general structure of the equation. \n",
        "\n",
        "If the sum doesn't look familar to you yet, we can recast it as a dot product:\n",
        "\n",
        "$$\\mathbf{W} \\cdot \\mathbf{X}$$<br>\n",
        "where $W$ is a $n\\times n$ convolution filter and $X$ is an  underlying $n\\times n$ region of the image we are applying the convolution to. <br><br>\n",
        "This matrix dot product is a scalar, which you recognize as the computation for a pixel value  in the activation map.<br><br>\n",
        "If for example, $n=2$,  then the activation pixel value is the sum of the four terms<br><br>\n",
        "\n",
        "$$\\mathbf{W} \\cdot \\mathbf{X} = W_{11} X_{11} + W_{12} X_{12}+ W_{22} X_{22} + W_{21} X_{21}$$<br>\n",
        "\n",
        "In a convolution, one can add a bias term...<br><br>\n",
        "\n",
        "$$\\mathbf{W} \\cdot \\mathbf{X} + b$$<br>\n",
        "\n",
        "And optionally, an activation function...<br><br>\n",
        "\n",
        "$$\\sigma(\\mathbf{W} \\cdot \\mathbf{X} + b)$$<br>\n",
        "\n",
        "So the output would look like this:<br><br>\n",
        "\n",
        "$$y~=~\\sigma(\\mathbf{W} \\cdot \\mathbf{X} + b),$$<br><br>\n",
        "Where $y$ is the value of a pixel in the activation map\n",
        "\n",
        "The convolutional layer is composed of neurons (perceptrons) that specialize in processing visual information.\n",
        "\n",
        "Now `scipy.ndimage.convolve` doesn't use an activation function or a bias term, but the convolutional layers that we'll be using in our CNNs do have a bias term and (optionally) an activation function. \n",
        "\n",
        "Do you see how ubiquitous that perceptron equation is? This is why we took the time to learn it in Sprint 2.\n",
        "\n",
        "_To recap: the perceptron is the fundamental building block of a neural network_"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IRndYGj-NpYd"
      },
      "source": [
        "### Feature Maps \n",
        "\n",
        "![](https://www.researchgate.net/profile/Mehdi-Cherti/publication/326412238/figure/fig29/AS:648874402271233@1531715121461/Example-of-feature-hierarchy-learned-by-a-deep-learning-model-on-faces-from-Lee-et-al.png)\n",
        "\n",
        "\n",
        "The matrix that is created by a convolution is a called **Feature Map**. \n",
        "\n",
        "Feature maps are matrices that contain the output of a convolutional layer. \n",
        "\n",
        "Each convolutional layer is performing both dimensionality reduction and feature engineering. \n",
        "\n",
        "In order to understand the above images, let's focus on the feature engineering part. \n",
        "\n",
        "**1st layer's job** is to detect lines/edges from the raw pixel values, then pass those lines (features that were created by the convolutional neurons) to the next layer. \n",
        "\n",
        "**2nd layer's job** is to accept the lines and combine them (like lego blocks) to create new features with them, this time parts of an object. Those parts are then passed forward to the next convolutional layer. \n",
        "\n",
        "**3rd layer's job** is to accept the parts of an object and put them together (like lego blocks) and create even more complex features, like a person's face, a car, a building, an animal, any object contained in the dataset. \n",
        "\n",
        "\n",
        "References: \n",
        "\n",
        "[Feature Visualization How neural networks build up their understanding of images](https://distill.pub/2017/feature-visualization/)\n",
        "\n",
        "[Visualizing and Understanding Convolutional Networks, by\n",
        "Matthew D Zeiler, Rob Fergus](https://arxiv.org/abs/1311.2901)\n",
        "\n",
        "[A guide to convolution arithmetic for deep learning](https://arxiv.org/abs/1603.07285) From the abstract: \"The guide clarifies the relationship between various properties (input shape, kernel shape, zero padding, strides and output shape) of convolutional, pooling and transposed convolutional layers, as well as the relationship between convolutional and transposed convolutional layers. Relationships are derived for various cases, and are illustrated in order to make them intuitive. \""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zDaTY2uSHQhJ"
      },
      "source": [
        "------\n",
        "\n",
        "### Pooling Layer\n",
        "\n",
        "![](https://lambdaschool-data-science.s3.amazonaws.com/images/Unit4/Sprint2/Module2/Screen+Shot+2020-02-25+at+10.26.13+AM.png)\n",
        "\n",
        "*Image Credits from __Hands on Machine Learning with Sckit-Learn, Keras & TensorFlow__*\n",
        "\n",
        "We use Pooling Layers to reduce the dimensionality of the feature maps.<br>\n",
        "We get smaller and smaller feature sets by repeatedly apply convolutions followed by pooling layers. \n",
        "\n",
        "Let's take a look at a simple example using Austen's photo."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eAePA6xCHsn7"
      },
      "source": [
        "plt.imshow(austen_grayscale, cmap=\"gray\");\n",
        "print(austen_grayscale.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PUcpIHm_HQhK"
      },
      "source": [
        "from skimage.measure import block_reduce\n",
        "\n",
        "reduced1 = block_reduce(austen_grayscale, (2,2), np.max)\n",
        "plt.imshow(reduced1, cmap=\"gray\");\n",
        "print(reduced1.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fCpazPNDoBGD"
      },
      "source": [
        "With MaxPooling of (2,2) we are able to reduce the size of our image \n",
        "by a factor of 4 with out noticably losing any important information. <br>\n",
        "We still preserve the light contrast and the lines, and much of the detail on Austen's face"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RtNFHn-qTnLZ"
      },
      "source": [
        "reduced2 = block_reduce(reduced1, (2,2), np.max)\n",
        "plt.imshow(reduced2, cmap=\"gray\");\n",
        "print(reduced2.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Omp05SlPTyu7"
      },
      "source": [
        "reduced3 = block_reduce(reduced2, (2,2), np.max)\n",
        "plt.imshow(reduced3, cmap=\"gray\");\n",
        "print(reduced3.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0Z37Pz6CT1ZL"
      },
      "source": [
        "reduced4 = block_reduce(reduced3, (2,2), np.max)\n",
        "plt.imshow(reduced4, cmap=\"gray\");\n",
        "print(reduced4.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "54uybdqXnQZd"
      },
      "source": [
        "The motivation behind the pooling layer is to reduce the number \n",
        "of trainable parameters by downsampling, <br>\n",
        "while still preserving essential features of the image."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z0IO7nP9HQhN"
      },
      "source": [
        "## Challenge\n",
        "\n",
        "You will be expected to be able to describe convolution. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qty8NQlnHQhN"
      },
      "source": [
        "---------\n",
        "\n",
        "# 2. CNNs for Classification (Learn)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QK6ZUOQZHQhO",
        "toc-hr-collapsed": true
      },
      "source": [
        "## Overview"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OOep4ugw8coa"
      },
      "source": [
        "### Typical CNN Architecture\n",
        "\n",
        "![A Typical CNN](https://upload.wikimedia.org/wikipedia/commons/thumb/6/63/Typical_cnn.png/800px-Typical_cnn.png)\n",
        "\n",
        "The first stage of a CNN is, unsurprisingly, a convolution - specifically, a transformation that maps regions of the input image to neurons responsible for receiving them. The convolutional layer can be visualized as follows:\n",
        "\n",
        "![Convolutional layer](https://upload.wikimedia.org/wikipedia/commons/6/68/Conv_layer.png)\n",
        "\n",
        "The red represents the original input image, and the blue the neurons that correspond.\n",
        "\n",
        "As shown in the first image, a CNN can have multiple rounds of convolutions, [downsampling](https://en.wikipedia.org/wiki/Downsampling_(signal_processing)) (a digital signal processing technique that effectively reduces the information by passing through a filter), and then eventually a fully connected neural network and output layer. Typical output layers for a CNN would be oriented towards classification or detection problems - e.g. \"does this picture contain a cat, a dog, or some other animal?\"\n",
        "\n",
        "\n",
        "#### A Convolution in Action\n",
        "\n",
        "![Convolution](https://miro.medium.com/max/1170/1*Fw-ehcNBR9byHtho-Rxbtw.gif)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iBJVnvdBlb3N"
      },
      "source": [
        "#### [Stanford CS231 Convolutions](https://cs231n.github.io/convolutional-networks/) has some great material, including an enlightening animation!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RA_8-zDNHQhP"
      },
      "source": [
        "------\n",
        "# Build a CNN\n",
        "For this exercise we'll use the [CIFAR10 small images classification dataset](https://keras.io/api/datasets/cifar10/), available in Keras.<br>\n",
        "CIFAR10 is a set of 50,000 training and 10,000 test images. <br>\n",
        "The images are in 10 classes, as with our MNIST and Fashion MNIST data sets<br>\n",
        "What's different is these are color images; each example is a tensor of shape 32x32x3 pixels.<br>\n",
        "You can think of each example as composed of three 32x32 images, one each in Red, Green and Blue (RGB) filters.\n",
        "\n",
        "Reference: [Interactive CIFAR model built in JavaScript](https://cs.stanford.edu/people/karpathy/convnetjs/demo/cifar10.html) built by Andrej Karpathy."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KWKwH7DIHQhP"
      },
      "source": [
        "from tensorflow.keras import datasets\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Conv2D, MaxPooling2D, Flatten"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G25xnWibNpYj"
      },
      "source": [
        "### First let's load and explore the data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O6HR5Jz4HQhR"
      },
      "source": [
        "# load in our color images\n",
        "(train_images, train_labels), (test_images, test_labels) = datasets.cifar10.load_data()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "nbgrader": {
          "grade": false,
          "grade_id": "cell-caf1d29aa68b3ca1",
          "locked": false,
          "schema_version": 3,
          "solution": true,
          "task": false
        },
        "id": "h6qtTmv6NpYk"
      },
      "source": [
        "# Normalize pixel values between 0 and 1\n",
        "# this is done by dividing by the max pixel value \n",
        "max_pixel_value = train_images[0].max()\n",
        "print(max_pixel_value)\n",
        "train_images, test_images = train_images / max_pixel_value, test_images / max_pixel_value"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o8qu6KtQHQhT"
      },
      "source": [
        "class_names = ['airplane', \n",
        "               'automobile', \n",
        "               'bird', \n",
        "               'cat', \n",
        "               'deer',\n",
        "               'dog', \n",
        "               'frog', \n",
        "               'horse', \n",
        "               'ship', \n",
        "               'truck']\n",
        "\n",
        "plt.figure(figsize=(10,10))\n",
        "for i in range(25):\n",
        "    plt.subplot(5,5,i+1)\n",
        "    plt.xticks([])\n",
        "    plt.yticks([])\n",
        "    plt.grid(False)\n",
        "    plt.imshow(train_images[i], cmap=plt.cm.binary)\n",
        "    # The CIFAR labels happen to be arrays, \n",
        "    # which is why you need the extra index\n",
        "    plt.xlabel(class_names[train_labels[i][0]])\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2oVcQhlGNpYl"
      },
      "source": [
        "## Tensors\n",
        "\n",
        "![](https://miro.medium.com/max/891/0*jGB1CGQ9HdeUwlgB)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-Yh04xEGHQhV"
      },
      "source": [
        "# this is a Rank 3 tensor \n",
        "# another way of thinking about this array is that it is 3-D stack of 2-D tensors\n",
        "train_images[0].shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lLsZ6GM_IhMV"
      },
      "source": [
        "# this is a Rank 4 tensor \n",
        "# think of this array as a stack of 3D images\n",
        "train_images.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x8vJRpVVNpYn"
      },
      "source": [
        "_____\n",
        "\n",
        "### Build a CNN Model to process CIFAR10 images, using Keras"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "nbgrader": {
          "grade": false,
          "grade_id": "cell-5a304de2902938ff",
          "locked": false,
          "schema_version": 3,
          "solution": true,
          "task": false
        },
        "id": "KRVIh7Y6NpYn"
      },
      "source": [
        "# this is the number of labels that we want to predict in the output layer\n",
        "n_features = len(np.unique(train_labels))\n",
        "n_features"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aa5ZcbSWHQha",
        "nbgrader": {
          "grade": false,
          "grade_id": "cell-5e50d5c53fbd3f6c",
          "locked": false,
          "schema_version": 3,
          "solution": true,
          "task": false
        }
      },
      "source": [
        "# build model layer by layer\n",
        "\n",
        "###BEGIN SOLUTION\n",
        "\n",
        "# keras calls them filters and kernels, we call them weight matrices \n",
        "# weight matrices have cell values \n",
        "# weight matrix cell values are randomly initialized \n",
        "# and get updated during Gradient Descent just like weights in the FCFF network \n",
        "n_weight_matrices = 32\n",
        "\n",
        "# specify the window size (i.e. 3 cell high and 3 cell wide)\n",
        "# convolution kernel, convolution filter, weight matrix\n",
        "weight_matrix_size = (3,3)\n",
        "\n",
        "# output of the convolutions between the weight matrices and the image pixels \n",
        "# will be passed into the activation function (if activate function isn't None)\n",
        "# ie. y = f(w*x + b)\n",
        "act_func = 'relu'\n",
        "\n",
        "# dim of the image: 32 cell high, 32 cell wide, and 3 channels (one for Red, one for Blue, and one for Green)\n",
        "# this means our images are not matrices, they are tensors \n",
        "image_dim = (32,32,3)\n",
        "\n",
        "pool_size = (2,2)\n",
        "\n",
        "model = Sequential()\n",
        "\n",
        "### Note about the weights in the convolutional layers ####\n",
        "# during training, the weights in the weight matrix (i.e. the windows used for convolutions) are updated \n",
        "# these weights are responsible for identifying important features in the images (i.e. feature engineering)\n",
        "# they must be tuned so that each convolutional layer is able to identify features (i.e. feature engineering)\n",
        "# each layer creates features, then pass those features to the next layer so that the next layer can use those features to create new features (hieratical features!)\n",
        "\n",
        "\n",
        "# 1st conv layer \n",
        "model.add(Conv2D(n_weight_matrices, \n",
        "                 weight_matrix_size, \n",
        "                 activation=act_func, \n",
        "                 input_shape=image_dim))\n",
        "\n",
        "# 1st pooling layer \n",
        "model.add(MaxPooling2D(pool_size))\n",
        "\n",
        "# 2nd conv layer \n",
        "model.add(Conv2D(128,\n",
        "                 weight_matrix_size, \n",
        "                 activation=act_func))\n",
        "\n",
        "# 2nd pooling layer \n",
        "model.add(MaxPooling2D(pool_size))\n",
        "\n",
        "# 3rd conv layer \n",
        "model.add(Conv2D(64, \n",
        "                 weight_matrix_size, \n",
        "                 activation=act_func))\n",
        "\n",
        "# not adding a 3rd pooling layer becaue the size of the image at this point\n",
        "# is already very small, i.e. (4,4)\n",
        "\n",
        "# flatten the image matrix into a row vector \n",
        "model.add(Flatten())\n",
        "\n",
        "# hidden layer in FCFF portion of model \n",
        "model.add(Dense(60, activation=act_func))\n",
        "\n",
        "# hidden layer in FCFF portion of model \n",
        "model.add(Dense(10, activation=\"softmax\"))\n",
        "\n",
        "model.summary()\n",
        "###END SOLUTION"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hnSape5Mkair"
      },
      "source": [
        "### Questions about the CNN we just built"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d8ELLlPbfuOj"
      },
      "source": [
        "Q1: Can you explain the Output Shape of each of these layers?<br>\n",
        "* `conv2d_9`\n",
        "* `max_pooling2d_6`\n",
        "* `conv2d_10`\n",
        "* `max_pooling2d_7`\n",
        "* `conv2d_11` \n",
        "* `flatten_3` \n",
        "\n",
        "Q2: Can you explain the number of parameters in each layer?<br>\n",
        "Hint: for `conv2d_9` there are $32$ $(3\\times3)$ convolution filters for each of the $3$ (RGB) input images. You can think of these convolution filters as a stack of $3\\times3\\times3$ tensors, each of which is applied to the $32\\times32\\times3$ input image. There are a total of $32\\times3\\times3\\times3$ = $864$ weights. And each of these $3\\times3\\times3$ convolution filters also has an associated bias, so there are a total of $864 + 32 = 896$ parameters.\n",
        "\n",
        "Q3: How could you make the activation map produced by each convolution the same size as its input, instead of $2$ pixels smaller in each dimension? Try it. Does this improve the model's performance?\n",
        "\n",
        "Q4: Why does the last `Dense` layer have 10 neurons?\n",
        "\n",
        "Q5: Why does the last `Dense` layer have a different activation function than the previous `Dense` layer?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6fe9kvNM92k-"
      },
      "source": [
        "# import Adam in format that allows tuning the learning rate\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "# specify learning rate and optimizer\n",
        "opt = Adam(learning_rate=0.001)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4Niy6az_HQhc"
      },
      "source": [
        "# Compile Model\n",
        "model.compile(optimizer=opt,\n",
        "              loss='sparse_categorical_crossentropy',\n",
        "              metrics=['accuracy'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "e5_NmqZHHQhd",
        "jupyter": {
          "outputs_hidden": true
        }
      },
      "source": [
        "%%time\n",
        "# Fit Model\n",
        "model.fit(train_images, \n",
        "          train_labels, \n",
        "          epochs=10, batch_size=128,\n",
        "          validation_data=(test_images, test_labels)\n",
        "          )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T8JXvZjDHQhf"
      },
      "source": [
        "# Evaluate Model\n",
        "test_loss, test_acc = model.evaluate(test_images, test_labels, verbose=2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8NC-QItONpYp"
      },
      "source": [
        "## Challenge\n",
        "\n",
        "You will apply CNNs to a classification task in the module project."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8XL-kM_GNpYp"
      },
      "source": [
        "-------\n",
        "### Digression\n",
        "\n",
        "We do have the option of taking a look at the weight values (from the convolutional layers and FCFF layers). The input data (the images) are normalized to values between 0 and 1. Our lesson on Gradient Descent (from Sprint 2) taught us that normalized data should have 2 observable effects: \n",
        "\n",
        "- The weight values should all be about the same size (i.e. on the same order of magnitude). \n",
        "- The model performance should be better when using normlized data than on non-normalized data set. \n",
        "\n",
        "A fun experiment you should consider running at some other time would be to see if these observations are reversed and to what extent if the input data is not normalized. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "nbgrader": {
          "grade": false,
          "grade_id": "cell-b23fb65b34cd468a",
          "locked": false,
          "schema_version": 3,
          "solution": true,
          "task": false
        },
        "id": "UP2KN2tlNpYq"
      },
      "source": [
        "\n",
        "# model has a bunch of cool attributes that you can explore\n",
        "# one such attribute is the tuned weights between each layer \n",
        "weights = model.get_weights()\n",
        "\n",
        "# these are the trained/learned weights of our model when we NORMALIZE our input data\n",
        "# EXPERIMENT: what would the weight values be if we DIDN'T NORMALIZE our input data?\n",
        "for w_mat in weights:\n",
        "    print (w_mat.min(), w_mat.max())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vO0ABrjeNpYq"
      },
      "source": [
        "-------"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tGFiPYCVHQhh"
      },
      "source": [
        "# 3. Transfer Learning for Image Classification (Learn)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-rCyTJl2HQhh",
        "toc-hr-collapsed": true
      },
      "source": [
        "## Overview"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ic_wzFnprwXI"
      },
      "source": [
        "### Transfer Learning Repositories\n",
        "\n",
        "#### [TensorFlow Hub](https://www.tensorflow.org/hub/) \n",
        "is a library for reusable machine learning models, that lets you quickly take advantage of models that were trained with thousands of GPU hours. \n",
        "\n",
        "This makes possible [**transfer learning**](https://keras.io/guides/transfer_learning/) - reusing a trained model's weights and biases as your model's initial values, then continuing to train the model with your own data to arrive at the best values for your task. The advantages are fairly clear - you can use less training data, have faster training, and have a model that generalizes better. It works because a model that is trained well on a large number of images has already learned the basic components common to these images. So if the images in your own data set are not too different than the ones in the original large training data, the parametes from the trained model will work reasonably well on your images \"right out of the box\". Adopting the trained weights and biases as your intial values, then continuing to train the model can often significantly improve the performance. Note that transfer learning requires that your model architecture is identical to that of the original trained model!\n",
        "\n",
        "\n",
        "\n",
        "TensorFlow Hub is very bleeding edge, and while there's a good amount of documentation out there, it's not always updated or consistent. You'll have to use your problem-solving skills if you want to use it!\n",
        "\n",
        "#### Keras API - Applications\n",
        "\n",
        "> Keras Applications are deep learning models that are made available alongside pre-trained weights. These models can be used for prediction, feature extraction, and fine-tuning.\n",
        "\n",
        "There is a decent selection of important benchmark models. \n",
        "\n",
        "We'll focus on a workhorse image classifier: **ResNet50.**\n",
        "\n",
        "Here's a link that lists all the pre-trained models in the [**Keras Library**](https://keras.io/api/applications/)\n",
        "\n",
        "The ImageNet competition started in 2010 and has a labeled dataset consisting of 1.2 million images, each a member of one of 1000 classes. This competition has played a key role in spurring theoretical advances in Computer Vision. [**paperswithcode**](https://paperswithcode.com/) has an interesting webpage [**Image Classification on ImageNet**](https://paperswithcode.com/sota/image-classification-on-imagenet) that keeps track of the evolution of the best-performing image classification architectures over the last 10 years or so. <br>\n",
        "\n",
        "\n",
        "Note the recent trend that [**Transformer**](https://jalammar.github.io/illustrated-transformer/)-based architectures -- which were developed for use in  Natural Language Processing -- are now performing as well as state of the art CNNs on Computer Vision problems! If you are interested in learning about Transformers, see the [**Hugging Face Transformers Course**](https://huggingface.co/course/chapter1)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NhF4z1aSHQhi"
      },
      "source": [
        "## Using a pre-trained model for image classification (Follow Along)\n",
        "We are going to classify images using a pretrained `ResNet50` model right \"out of the box\"."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FM_ApKbGYM9S"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "from tensorflow.keras.applications.resnet50 import ResNet50\n",
        "from tensorflow.keras.preprocessing import image\n",
        "from tensorflow.keras.applications.resnet50 import preprocess_input, decode_predictions"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y3zd5ud-NpYs"
      },
      "source": [
        "def process_img_path(img_path):\n",
        "    \"\"\"\n",
        "    Using tensorflow per-build image processor. \n",
        "\n",
        "    Returns processed image. \n",
        "    \"\"\"\n",
        "    # docs: https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/image/load_img\n",
        "    return image.load_img(img_path, target_size=(224, 224))\n",
        "\n",
        "# Decide whether any of the three most likely image classes is \"banana\"\n",
        "def img_contains_banana(img):\n",
        "    \"\"\"\n",
        "    Imputs image into resnet50 pre-trained model and returns the top 3 likely labels for the image (ranked by largest probability)\n",
        "    \"\"\"\n",
        "    # preprocess image\n",
        "    x = image.img_to_array(img)\n",
        "    x = np.expand_dims(x, axis=0)\n",
        "    x = preprocess_input(x)\n",
        "    print('x.shape after preprocess_input ',x.shape)\n",
        "    \n",
        "    # instantiate pretrained ResNet50 model\n",
        "    model = ResNet50(weights='imagenet')\n",
        "    \n",
        "    # get classification of image\n",
        "    features = model.predict(x)\n",
        "    print('shape of predictions ',features.shape)\n",
        "    \n",
        "    # docs: https://www.tensorflow.org/api_docs/python/tf/keras/applications/imagenet_utils/decode_predictions\n",
        "    results = decode_predictions(features, top=3)[0]\n",
        " \n",
        "    # return True if 'banana' is among the top 3 predictions\n",
        "    detected_banana = False\n",
        "    for entry in results:\n",
        "        print(entry)\n",
        "        if entry[1] == 'banana':\n",
        "            detected_banana = True\n",
        "    return detected_banana"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bpXyMD3PeBPS"
      },
      "source": [
        "### Let's have a look at the ResNet50 model architecture\n",
        "We imported the ResNet50 model, including the $25,583,592$ parameters (weights and biases) that were trained using the ImageNet data set!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wACrRAbKc3mC"
      },
      "source": [
        "model = ResNet50(weights='imagenet')\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3WiipYECp_on"
      },
      "source": [
        "Now let's use our `ResNet50` model to classify a few images and see whether it thinks they are a banana. <br>\n",
        "Our `img_contains_banana` function returns `True` if any of the top 3 predicted classes is `banana`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_cQ8ZsJF_Z3B"
      },
      "source": [
        "import requests\n",
        "\n",
        "# links to two images that we'll use with our pre-trained model\n",
        "image_urls = [\"https://github.com/LambdaSchool/ML-YouOnlyLookOnce/raw/master/sample_data/negative_examples/example11.jpeg\",\n",
        "              \"https://github.com/LambdaSchool/ML-YouOnlyLookOnce/raw/master/sample_data/positive_examples/example0.jpeg\"]\n",
        "\n",
        "for _id,img in enumerate(image_urls): \n",
        "    r = requests.get(img)\n",
        "    with open(f'example{_id}.jpg', 'wb') as f:\n",
        "        f.write(r.content)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gxzkai0q_d-4"
      },
      "source": [
        "from IPython.display import Image\n",
        "Image(filename='./example0.jpg', width=600)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X8NIlClb_n8s"
      },
      "source": [
        "processed_image = process_img_path('example0.jpg')\n",
        "\n",
        "img_contains_banana(processed_image)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YIwtRazQ_tQr"
      },
      "source": [
        "Image(filename='example1.jpg', width=600)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GDXwkPWOAB14"
      },
      "source": [
        "img_contains_banana(process_img_path('example1.jpg'))"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}