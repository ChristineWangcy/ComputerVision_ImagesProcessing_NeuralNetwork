{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "celltoolbar": "Create Assignment",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "colab": {
      "name": "Restore Damage images Autoencoders (Tensorflow, keras, Convolutional Nerual Network).ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "private_outputs": true
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "5h-Q6HVTDh5N"
      },
      "source": [
        "from IPython.display import YouTubeVideo\n",
        "YouTubeVideo('EjVzjxihGvU', width=1000, height=500)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qOOpqaDZDh5R"
      },
      "source": [
        "import os\n",
        "import random\n",
        "import numpy as np\n",
        "from numpy.random import normal\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "\n",
        "# need these layers for 1st section\n",
        "from tensorflow.keras.layers import Input, Dense, Conv2D, MaxPooling2D, UpSampling2D\n",
        "from tensorflow.keras.models import Model\n",
        "\n",
        "from sklearn.utils import shuffle\n",
        "from keras.datasets.mnist import load_data\n",
        "\n",
        "\n",
        "# need these layers for 2nd section\n",
        "from keras import layers\n",
        "from keras.layers import Reshape, Conv2DTranspose, Flatten\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5nEjxXGTDh5S"
      },
      "source": [
        "# Application of Auto-Encoders\n",
        "\n",
        "\n",
        "# Restore Damaged MNIST Dataset\n",
        "\n",
        "Our task is to build a model that restores damaged images, namely damaged MNIST digits. Because this is an exercise, we'll have to damage the images ourselves by adding random noise from a normal distribution. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2DOD0FjvDh5S"
      },
      "source": [
        "### Load the MNIST Image data, using the function provided by keras\n",
        "Then normalize the data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BM8XjMTpDh5T"
      },
      "source": [
        "# load in mnist dataset \n",
        "(x_train, y_train), (x_test, y_test) = load_data()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "nbgrader": {
          "grade": false,
          "grade_id": "cell-19c2b92ae584afbd",
          "locked": false,
          "schema_version": 3,
          "solution": true,
          "task": false
        },
        "id": "KJQVcSQWDh5T"
      },
      "source": [
        "# normalize pixel values between 0 and 1 for both the train and test set \n",
        "# save results to `x_train_norm` and `x_test_norm`\n",
        "\n",
        "###BEGIN SOLUTION\n",
        "x_train_norm = x_train / x_train.max()\n",
        "x_test_norm = x_test / x_test.max()\n",
        "###END SOLUTION"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s_FrllsNDh5U"
      },
      "source": [
        "assert x_train_norm.max() == 1.0, \"Did you normalized your training set?\"\n",
        "assert x_test_norm.max() == 1.0, \"Did you normalized your test set?\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OHZ40OGTG0Qs"
      },
      "source": [
        "x_train_norm.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CnI_cM96Dh5V"
      },
      "source": [
        "### Visualize Original and Damaged Images"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GEvYuqOxDh5V"
      },
      "source": [
        "# here we can see the first 10 original images from the test set \n",
        "plt.figure(figsize=(18,5))\n",
        "# helper function used to plot images \n",
        "for index in range(10):\n",
        "    plt.subplot(2,5, index+1)\n",
        "    plt.imshow(x_test_norm[index], cmap=plt.cm.binary)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FFCmpexhDh5W"
      },
      "source": [
        "### Damage Our Images\n",
        "\n",
        "We'll damage the images by adding noise to each pixel. <br>\n",
        "The noise will be samples from a normal (Gaussian) distribution, <br>\n",
        "We use the imported `numpy.random.normal` to generate the samples.<br><br>\n",
        "We'll generate noise arrays `x_train_noise` and `x_test_noise` and add them to<br> `x_train_norm` and `x_test_norm` respectively to get the noisy images.<br>\n",
        "The noise arrays must be the same size as the image arrays to which they will be added. <br>\n",
        "\n",
        "Look up the documentation of `numpy.random.normal` to find out how to generate noise arrays of the required shapes.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "nbgrader": {
          "grade": false,
          "grade_id": "cell-66a89e39bed9bdc1",
          "locked": false,
          "schema_version": 3,
          "solution": true,
          "task": false
        },
        "id": "nnDqIwaHDh5W"
      },
      "source": [
        "# parameters of the normal distribution to sample from \n",
        "mean = 0.5\n",
        "stddev = 0.3\n",
        "\n",
        "###BEGIN SOLUTION\n",
        "import numpy\n",
        "import cv2\n",
        "# create noise arrays x_train_noise and x_test_noise by sampling \n",
        "#    from a normal distribution with the given mean and stddev\n",
        "x_train_noise = numpy.random.normal(mean, stddev, 28*28).reshape(28,28)\n",
        "x_test_noise = numpy.random.normal(mean, stddev, 28*28).reshape(28,28)\n",
        "x_train_noise.shape\n",
        "# create the noisy train data by adding the appropriate noise array to x_train_norm\n",
        "x_train_noisy = x_train + x_train * x_train_noise\n",
        "# create the noisy test data by adding the appropriate noise array to x_test_norm\n",
        "x_test_noisy = x_test + x_test * x_test_noise\n",
        "###END SOLUTION"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MiHQtk3gqaTK"
      },
      "source": [
        "### Visualize the damaged images"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ScbfGmH2Dh5X"
      },
      "source": [
        "You should see highly grainy images of hand written digits below. By normalizing our images and adding noise sampled from a normal distribution centered around 0.5, we have created damaged images. Now our next task is to build an auto-encoder that learns how to restore the original images. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B7FcpjYYDh5X"
      },
      "source": [
        "plt.figure(figsize=(18,5))\n",
        "\n",
        "# helper function used to plot images \n",
        "for index in range(10):\n",
        "    plt.subplot(2,5, index+1)\n",
        "    plt.imshow(x_test_noisy[index], cmap=plt.cm.binary)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OR7WFuHUDh5X"
      },
      "source": [
        "### Build an Image Restoration Auto-Encoder\n",
        "\n",
        "You saw in the guided project that we can build an Auto-Encoder using Fully Connected Feed Forward (FCFF) layers; in Keras they're called Dense layers. Or we can build a Convolutional Auto-Encoder by using Conv2D and MaxPool2D layers. \n",
        "\n",
        "You are encouraged to experiment and build out an architecture of your choosing. However using a Convolutional Auto-Encoder is encouraged since that architecture is specifically designed for image data. \n",
        "\n",
        "Reference today's guided project for examples of how to build an Auto-Encoder. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9QP3Hn8RExem"
      },
      "source": [
        "# Create an input layer using Input() class\n",
        "\n",
        "# Create an encoder model \n",
        "\n",
        "# Create a decoder model \n",
        "\n",
        "# bring it all together by using the Model() class - save result to `restore_model`\n",
        "\n",
        "###BEGIN SOLUTION\n",
        "\n",
        "# Create input layer \n",
        "input_img = Input(shape=(28, 28, 1))\n",
        "\n",
        "# Encoder\n",
        "x = layers.Conv2D(32, (3, 3), activation=\"relu\", padding=\"same\")(input_img)\n",
        "x = layers.MaxPooling2D((2, 2), padding=\"same\")(x)\n",
        "x = layers.Conv2D(32, (3, 3), activation=\"relu\", padding=\"same\")(x)\n",
        "x = layers.MaxPooling2D((2, 2), padding=\"same\")(x)\n",
        "\n",
        "# Decoder\n",
        "x = layers.Conv2DTranspose(32, (3, 3), strides=2, activation=\"relu\", padding=\"same\")(x)\n",
        "x = layers.Conv2DTranspose(32, (3, 3), strides=2, activation=\"relu\", padding=\"same\")(x)\n",
        "x = layers.Conv2D(1, (3, 3), activation=\"sigmoid\", padding=\"same\")(x)\n",
        "\n",
        "restore_model = Model(input_img, x)\n",
        "restore_model.compile(optimizer=\"adam\", loss='binary_crossentropy')\n",
        "restore_model.summary()\n",
        "\n",
        "###END SOLUTION"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KIVStfT-Dh5Z"
      },
      "source": [
        "### Compile model "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "nbgrader": {
          "grade": false,
          "grade_id": "cell-04d7ed3d4d8bfec0",
          "locked": false,
          "schema_version": 3,
          "solution": true,
          "task": false
        },
        "id": "zMOm2-cyDh5Z"
      },
      "source": [
        "# compile model \n",
        "# use `mse` for the loss \n",
        "# use `nadam` as the optimizer \n",
        "\n",
        "###BEGIN SOLUTION\n",
        "from tensorflow.keras.metrics import mean_squared_error\n",
        "restore_model.compile(optimizer='nadam', loss='binary_crossentropy', metrics = [mean_squared_error])\n",
        "###END SOLUTION"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hikOQnSkDh5Z"
      },
      "source": [
        "### Fit model\n",
        "We will train the autoencoder to learn how to denoise a noisy image by using the uncorrupted image as the target!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "nbgrader": {
          "grade": false,
          "grade_id": "cell-b0bf2468df6bcdac",
          "locked": false,
          "schema_version": 3,
          "solution": true,
          "task": false
        },
        "id": "qPpyaEowDh5Z"
      },
      "source": [
        "%%time\n",
        "# fit model \n",
        "# use `x_train_noisy` as the input train data \n",
        "# use `x_train_norm` as the target\n",
        "# use 3 epochs \n",
        "# use the validation_data option in fit, passing in the test data\n",
        "# if you have access to multiple processors, set parameter `workers` to N - 1\n",
        "# where N is the total number of processors that you have \n",
        "\n",
        "###BEGIN SOLUTION\n",
        "from tensorflow.keras.callbacks import EarlyStopping, TensorBoard\n",
        "stop = EarlyStopping(monitor='val_loss', min_delta=0.001, patience=2)\n",
        "\n",
        "#  tensorboard callback\n",
        "logdir = os.path.join(\"logs\", f\"Autoencoder\")\n",
        "tensorboard = TensorBoard(log_dir=logdir)\n",
        "\n",
        "# fit the autoencoder model (train)\n",
        "restore_model.fit(x_train_noisy, # input image to encoder\n",
        "                  x_train_norm, # provide input image as the target, so that the model learns how to reconstruct the input image \n",
        "                  epochs=3,\n",
        "                  batch_size=64,\n",
        "                  validation_data=(x_test_noisy, x_test_norm),\n",
        "                  callbacks=[tensorboard],\n",
        "                  workers=10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eTLZ4djfDh5a"
      },
      "source": [
        "### Use our Autoencoder to Restore Damaged Images!\n",
        "\n",
        "Now that our autoencoder `restore_model` has been trained, we can use it to restore damaged images. \n",
        "\n",
        "Pass in `x_test_noisy` to the `.predict()` method, and save the results to `restored_imgs`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4mdeM9cHLG8n"
      },
      "source": [
        "x_test_noisy.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "nbgrader": {
          "grade": false,
          "grade_id": "cell-49247f9edc4892b0",
          "locked": false,
          "schema_version": 3,
          "solution": true,
          "task": false
        },
        "id": "bH8OnDzHDh5a"
      },
      "source": [
        "# restore damaged test set images \n",
        "###BEGIN SOLUTION\n",
        "restored_imgs = restore_model.predict(x_test_noisy)\n",
        "###END SOLUTION"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0v5v-6CzDh5a"
      },
      "source": [
        "### Visualize Damaged Images"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E5UN3xi_Dh5b"
      },
      "source": [
        "plt.figure(figsize=(18,5))\n",
        "\n",
        "# display the first 10 images from `x_test_noisy`\n",
        "for index in range(10):\n",
        "    plt.subplot(2,5, index+1)\n",
        "    plt.imshow(x_test_noisy[index], cmap=plt.cm.binary)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fymrOQ_6Dh5b"
      },
      "source": [
        "### Visualize Restored Images"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tm7pu7MuK-ju"
      },
      "source": [
        "restored_imgs[:,:,:,0].shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pyPrnDU_Dh5b"
      },
      "source": [
        "plt.figure(figsize=(18,5))\n",
        "\n",
        "# display the first 10 images from `restored_imgs`\n",
        "for index in range(10):\n",
        "    plt.subplot(2,5, index+1)\n",
        "    plt.imshow(restored_imgs[index,:,:,0], cmap=plt.cm.binary)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LOMCQW4IDh5b"
      },
      "source": [
        "### Conclusion \n",
        "\n",
        "Ok - so it worked! We were able to restored damaged images using an auto-encoder, applying the same essential idea as in the video from the top of the notebook. \n",
        "\n",
        "You might be thinking to yourself \"Wait a minute?! Doesn't this feel a little circular?In real life, we'd have only the damaged images, not the uncorrupted versions! What do we do then?\"\n",
        "\n",
        "Suppose you wanted to restore a set of damaged images, or perhaps an entire video (which is after all just an ordered set of images). One approach would be to find a way to restore a small number of images via some non-deep learning method, such as hiring an artist to do it manually.  Once you have a sufficient number pairs of damaged and restored images, you can create a training set and train a resstoration model. From that point on, the model can do the restoration in an automated fashion."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5V-KdozVDh5c"
      },
      "source": [
        "-----\n",
        "\n",
        "# Exercise 2: Introduction to Variational Auto-Encoders\n",
        "\n",
        "\n",
        "We will take the Standard Auto-Encoder framework that we have been playing with and change two things to make it even more powerful. To understand these changes, you will first watch the provided video; then, we'll break down the important bits in the notebook. \n",
        "\n",
        "Variational Autoencoders are **generative models**. A generative model learns a **probabilistic representation** of the data, <br>\n",
        "and is then capable of synthesizing an infinite variety of new examples from that distribution. Truly magic! <br><br>\n",
        "To experience the magic at first-hand, take a look at the page [This Person Does Not Exist](https://thispersondoesnotexist.com/). Every time you refresh the page, you are shown an image of an imaginary person! These images are  produced by another kind of generative model called a Generative Adversarial Network (GAN).<br><br>\n",
        "\n",
        "Although you are highly encouraged to experiment with the code provided, you will not be asked to fill in any missing code in this section. \n",
        "\n",
        "The questions at the end of this section that test your reading comprehension of the material on variational autoencoders.\n",
        "\n",
        "\n",
        "## First some Theory\n",
        "\n",
        "Watch the following video to get a crash course on the theory of Variational Auto-Encoders. \n",
        "\n",
        "The first part of the video is a great recap of the standard autoencoder.  The section introducing the Variational Auto Encoder (VAE) **starts at about 5:40 and ends at about 9:06**. Afterward, he discusses a more sophisticated version of VAE that is outside the scope of today's assignment -- still interesting, but optional.\n",
        "\n",
        "Don't stress out if you don't understand everything he is saying. Take note of the following: \n",
        "\n",
        "- Watch the VAE section of the video 2 - 3 times and absorb as much information as you can\n",
        "- We will unpack the important bits later in the notebook \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vUZtP6c0Dh5c"
      },
      "source": [
        "from IPython.display import YouTubeVideo\n",
        "YouTubeVideo('9zKuYvjFFS8', width=1000, height=500)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZuVxF4p4Dh5c"
      },
      "source": [
        "![](https://lh3.googleusercontent.com/proxy/9StAnnRiBvF4rNIMEXj2Qc5kGvWHQu7H6wOqxYI9wAPCN3Jy8JUE4awTyslXvFO2Etb2-yY8xgFvAH0zNMx8BUQmQ0Ca2FwgOw)\n",
        "\n",
        "Having watched the video, you'll see that there are a lot of technical details, but the most important ideas are:\n",
        "\n",
        "- The introduction of the **`z_mean`** and  **`z_log_var`** vectors \n",
        "- The two-part loss function with the **Reconstruction loss** and **KL loss** components \n",
        "\n",
        "Let's focus on the **`z_mean`** and  **`z_log_var`** vectors for now and we'll come back to the loss function later. \n",
        "\n",
        "Notice that the Variational Auto-Encoder is our standard Auto-Encoder plus the **`z_mean`** and  **`z_log_var`** vectors in the middle. \n",
        "\n",
        "Standard Auto-Encoders also represent data in a **latent vector** - it's simply the output of the last encoding layer!\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zGed0SEtDh5d"
      },
      "source": [
        "## Sampling the Normal Distribution in Latent Space\n",
        "\n",
        "### Latent Space \n",
        "\n",
        "A Latent Space is a representation of data in compressed form, i.e. representing high dimensional data in a low dimensional space. If your interest in piqued, read [Understanding Latent Space in Machine Learning](https://towardsdatascience.com/understanding-latent-space-in-machine-learning-de5a7c687d8d#:~:text=The%20latent%20space%20is%20simply,representations%20of%20data%20for%20analysis.)<br><br>\n",
        "\n",
        "### Latent N-dimensional Distribution \n",
        "\n",
        "![](https://theaisummer.com/assets/img/posts/Autoencoder/vae.png)\n",
        "\n",
        "\n",
        "The image below shows the architecture of a **Variational Auto-Encoder**. Notice that it looks similar to standard auto-encoder? <br>\n",
        "Except for those latent vectors in the middle. \n",
        "\n",
        "In the standard autoencoder, the latent vector is deterministically computed at the the output of the encoder mode.\n",
        "\n",
        "The basic idea of the VAE is to compute the latent vector by sampling from a Gaussian distribution with a mean and variance that are **parameters to be learned by the network**.\n",
        "\n",
        "![](https://bookdown.org/phamtrongthang123/notebookCEVAE/imgs/2020-04-12-10-49-37.png)\n",
        "\n",
        "$$\\mathbf{Z} = \\mathbf{\\mu} + \\mathbf{\\sigma} \\bigodot \\mathbf{\\epsilon}$$\n",
        "\n",
        "Where $\\epsilon$ is sampled from a multivariate standard normal distribution \n",
        "\n",
        "$$\\epsilon \\sim \\mathcal{N}(\\mu,\\,\\sigma^{2}) ~=~ \\mathcal{N}(0, 1) $$\n",
        "\n",
        "And $\\bigodot$ means matrix product. \n",
        "\n",
        "We know that **mean** and **variance** are just numbers, single floating-point values, right? <br>\n",
        "So how do we get vectors from these distribution parameters?\n",
        "The vectors come from an [**N-dimensional Normal distribution**](https://en.wikipedia.org/wiki/Multivariate_normal_distribution). <br>\n",
        "\n",
        "So far, we've  dealt mostly with univariate probability distributions, which are functions of a single variable. However, it is possible to have probability distributions that are functions of many variables! One example is the [**multinomial distribution**](https://en.wikipedia.org/wiki/Multinomial_distribution) which came up in Module 1 this week when we needed to sample from a set of N positive numbers that sum to 1. <br>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_9sY8oCfDh5d"
      },
      "source": [
        "class Sampling(layers.Layer):\n",
        "    \"\"\"\n",
        "    Uses (z_mean, z_log_var) to sample a z vector from the latent N-dimensional Normal distribution\n",
        "    i.e. Z is the vector encoding a digit.\n",
        "    \"\"\"\n",
        "    def call(self, inputs):\n",
        "        \n",
        "        # recall from the video that z_mean and z_log_var are vectors \n",
        "        z_mean, z_log_var = inputs\n",
        "        batch = tf.shape(z_mean)[0]\n",
        "        dim = tf.shape(z_mean)[1]\n",
        "        \n",
        "        # sample from an N-dimensional normal distribution\n",
        "        # epsilon is given shape (batch, dim) because we are adding it to z_mean which has shape (batch, dim)\n",
        "        epsilon = tf.keras.backend.random_normal(shape=(batch, dim))\n",
        "        \n",
        "        # output of tf.exp() is a vector \n",
        "        sigma = tf.exp(0.5 * z_log_var)\n",
        "        \n",
        "        # this is our hidden latent vector made up of a mean and variance vector \n",
        "        # variance vector is scaled by epsilon, which is sampled from a normal distribtuion \n",
        "        # When the guy in the video said \"stochastic\" in reference to epsilon, he meant random \n",
        "        Z = z_mean + sigma * epsilon\n",
        "        \n",
        "        # return hidden latent vector \n",
        "        return Z"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yy7ZTz42Dh5d"
      },
      "source": [
        "# Build a Variational Auto-Encoder model\n",
        "\n",
        "We'll build the Encoder model, followed by the Decoder model. <br>\n",
        "\n",
        "Then we'll put them together using Keras's Model API for building models, just like we did in the guided project. \n",
        "\n",
        "Note that the there is nothing special about the the Encoder or Decoder architectures we are using. You can experiment with different architectures after you've gone through the notebook once. \n",
        "\n",
        "## Build the Encoder Model\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QI_0brUdDh5e"
      },
      "source": [
        "# recall from the video that the more dimensions that our latent vector has,\n",
        "# the better the results of our model \n",
        "latent_dim = 2\n",
        "\n",
        "# shape of our input data\n",
        "# we are creating our input layer using Keras's Input() class\n",
        "# the only thing that input layers really do is define the dimensionality of the input data for the model\n",
        "encoder_inputs = Input(shape=(28, 28, 1))\n",
        "\n",
        "# these are the hidden layers of our encoder model \n",
        "# `x` is the output from each layer \n",
        "# recall that the data is in the shape of a matrix, hence Conv2D - 2D as in 2-dimensional \n",
        "x = Conv2D(32, 3, activation=\"relu\", strides=2, padding=\"same\")(encoder_inputs)\n",
        "x = Conv2D(64, 3, activation=\"relu\", strides=2, padding=\"same\")(x)\n",
        "# now flatten data matrix into data vector \n",
        "x = Flatten()(x)\n",
        "# pass data vector into FCFF layer \n",
        "x = Dense(16, activation=\"relu\")(x)\n",
        "\n",
        "# recall that ordinarily the output of the last encoding layer is the latent vector \n",
        "# but here we are creating two output layers for our encoder - one for the mean and one for the log variance \n",
        "# returns a 2-dim mean vector\n",
        "z_mean = Dense(latent_dim, name=\"z_mean\")(x)\n",
        "# returns a 2-dim log variance vector \n",
        "\n",
        "z_log_var = Dense(latent_dim, name=\"z_log_var\")(x)\n",
        "# pass mean and variance vector into Sampling class in order to create the Z vector,  Z = mean + var * epsilon\n",
        "z = Sampling()([z_mean, z_log_var])\n",
        "\n",
        "# this is our encoder model \n",
        "# inputs are the original images\n",
        "# outputs are the Z vectors: mean, log variance, and the complete Z, i.e. vector Z = mean + var * epsilon\n",
        "encoder = Model(inputs=encoder_inputs, outputs=[z_mean, z_log_var, z], name=\"encoder\")\n",
        "encoder.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Twph7CtBDh5e"
      },
      "source": [
        "## Build the Decoder Model\n",
        "\n",
        "Nexty we'll create the decoder model.\n",
        "\n",
        "There is a new layer here that we should talk about, it's the **Conv2DTranspose** layer. \n",
        "\n",
        "This is also known as a **Deconvolutional layer**. It's like the Convolutional layer that we learned about in Sprint 3 Module 2 but it \"moves\" in the opposite direction. \n",
        "\n",
        "\n",
        "### Deconvolution\n",
        "\n",
        "![](https://miro.medium.com/max/1086/1*AbCrAqPBfkqGRdhKtiZQqA.png)\n",
        "\n",
        "On the **left hand side** we see what a **Convolution** looks like (this should look familiar). The layer is taking the original image (or feature map) on the left and applying a convolution, the result is a feature map on the right with a smaller dimensionality. \n",
        "\n",
        "On the **right hand side** we see what a **Deconvolution** looks like. The layer is taking the feature map (the output of a convolution layer) and attempting to rebuild the original image. \n",
        "\n",
        "Recall that a  **Convolution**  slides a weight matrix over an image in order to create features, which are stored as an array called a **Feature Map.** \n",
        "\n",
        "The **Deconvolution**  layer starts with a **Feature Map** and tries to create the original image. \n",
        "\n",
        "Hence **Deconvolution** is like **Convolution**  just moving in the opposite direction. \n",
        "\n",
        "For a deeper explanation, see the article [**Convolutions: Transposed and Deconvolution**](https://medium.com/@marsxiang/convolutions-transposed-and-deconvolution-6430c358a5b6)\n",
        "\n",
        "\n",
        "Another reference is the [**Keras docs on on the Conv2dTranspose layer**](https://keras.io/api/layers/convolution_layers/convolution2d_transpose/)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lq54jJCoDh5e"
      },
      "source": [
        "# the input layer to our decoder has the same dimensionality as the latent vector\n",
        "# which is the input to the decoder\n",
        "latent_inputs = Input(shape=(latent_dim,))\n",
        "\n",
        "# these are the hidden layers of our decoder \n",
        "# the data at this point is in a vector, the Z latent vector \n",
        "x = Dense(7 * 7 * 64, activation=\"relu\")(latent_inputs)\n",
        "# reshape vector into a matrix\n",
        "x = Reshape((7, 7, 64))(x)\n",
        "\n",
        "# increase the size of the matrix by using Deconvolutional layers\n",
        "# notice that the number of feature maps (i.e. the first parameter values) are mirror images\n",
        "# of the number of feature maps from the Conv2D layers in the Encoder\n",
        "# we saw this mirroring in the auto-encoders we built in the guided project \n",
        "x = Conv2DTranspose(64, 3, activation=\"relu\", strides=2, padding=\"same\")(x)\n",
        "x = Conv2DTranspose(32, 3, activation=\"relu\", strides=2, padding=\"same\")(x)\n",
        "\n",
        "# this is the final layer in out decoder\n",
        "# therefore this layer outputs the reconstruction of the original image \n",
        "decoder_outputs = Conv2DTranspose(1, 3, activation=\"sigmoid\", padding=\"same\")(x)\n",
        "\n",
        "# this is our decoder model \n",
        "decoder = Model(inputs=latent_inputs, outputs=decoder_outputs, name=\"decoder\")\n",
        "decoder.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k9KxZY1gDh5g"
      },
      "source": [
        "## Combine Encoder & Decoder into a Variational Auto-Encoder \n",
        "\n",
        "Now that we've created the Encoder and Decoder models, we can combine them to build our Variational Auto-Encoder using a [keras custom model](https://keras.io/guides/customizing_what_happens_in_fit/).<br>\n",
        "\n",
        "In order to keep our code tidy, let's define the VAE model as a class. \n",
        "\n",
        "**Side Note:** \n",
        "\n",
        "You've probably noticed how much code needs to be written just to run a gridsearch experiment in Sprint 2, and now all the code that one needs to write to create a deep learning VAE model. Yeah, welcome to Deep Learning, folks. Deep Learning involves a lot of software engineering! Something to consider if you're considering spending a lot of time building Deep Learning models.  To that point, it is a Good Thing that you're exposed to all of this now, so that you can get an honest and proper taste of what this kind of work is really like!\n",
        "\n",
        "\n",
        "**Property Decorator Function**\n",
        "\n",
        "You might have noticed the python decorator above the `metrics()` method in the class below. \n",
        "\n",
        "```python\n",
        "    @property\n",
        "    def metrics(self):\n",
        "        \"\"\"\n",
        "        Returns all losses in a list\n",
        "        \"\"\"\n",
        "        return [\n",
        "            self.total_loss_tracker,\n",
        "            self.reconstruction_loss_tracker,\n",
        "            self.kl_loss_tracker,\n",
        "        ]\n",
        "```\n",
        "\n",
        "If you're not familiar with this decorator, check this out: [**Python @property decorator**](https://www.programiz.com/python-programming/property)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bzts_12UDh5g"
      },
      "source": [
        "class VAE(keras.Model):\n",
        "    def __init__(self, encoder, decoder, **kwargs):\n",
        "        \"\"\"\n",
        "        This class build a Variational Auto-Encoder. It accepts an Encoder and Decoder model as input. \n",
        "        \n",
        "        Note\n",
        "        ----\n",
        "        This VAE class is inheriting Keras's Model API so that it can use the Model class methods \n",
        "\n",
        "        \"\"\"\n",
        "        # how python 3 handles inheritance \n",
        "        super(VAE, self).__init__(**kwargs)\n",
        "        # set encoder model as class attribute\n",
        "        self.encoder = encoder\n",
        "        # set decoder model as class attribute \n",
        "        self.decoder = decoder\n",
        "        # set mean function as class attribute - this calculates the total loss\n",
        "        self.total_loss_tracker = keras.metrics.Mean(name=\"total_loss\")\n",
        "        # set mean function as class attribute - this calculates the reconstruction loss\n",
        "        self.reconstruction_loss_tracker = keras.metrics.Mean(name=\"reconstruction_loss\")\n",
        "        # set mean function as class attribute - this calculates the kl loss\n",
        "        self.kl_loss_tracker = keras.metrics.Mean(name=\"kl_loss\")\n",
        "\n",
        "    @property\n",
        "    def metrics(self):\n",
        "        \"\"\"\n",
        "        Returns all losses in a list\n",
        "        \"\"\"\n",
        "        return [\n",
        "            self.total_loss_tracker,\n",
        "            self.reconstruction_loss_tracker,\n",
        "            self.kl_loss_tracker,\n",
        "        ]\n",
        "\n",
        "    def train_step(self, data):\n",
        "        \"\"\"\n",
        "        Training our model via gradient descent and back-propagation \n",
        "        \"\"\"\n",
        "        \n",
        "        # we used tf.GradientTape() in Sprint 2 Module 2 to run Gradient Descent from scratch \n",
        "        with tf.GradientTape() as tape:\n",
        "            # pass input data into encoder model \n",
        "            # output of encoder model is the hidden state distribution parameters and hidden state vector \n",
        "            z_mean, z_log_var, z = self.encoder(data)\n",
        "            \n",
        "            # pass hidden state vector into decoder model \n",
        "            reconstruction = self.decoder(z)\n",
        "            \n",
        "            # calculate the reconstruction loss \n",
        "            reconstruction_loss = tf.reduce_mean(\n",
        "                tf.reduce_sum(\n",
        "                    keras.losses.binary_crossentropy(data, reconstruction), axis=(1, 2)\n",
        "                )\n",
        "            )\n",
        "            \n",
        "            # calculate the kl loss\n",
        "            #                (1 + z_sigma   - (z_mean)^2        - e^(z_sigma) ) \n",
        "            kl_loss = -0.5 * (1 + z_log_var - tf.square(z_mean) - tf.exp(z_log_var))\n",
        "            # recall that we used tf.reduce_sum() in Sprint 2 Module 4 assignment \n",
        "            # it takes the sum of the vector components \n",
        "            kl_loss = tf.reduce_mean(tf.reduce_sum(kl_loss, axis=1))\n",
        "            \n",
        "            # calculate the total loss by adding the two loss components \n",
        "            total_loss = reconstruction_loss + kl_loss\n",
        "            \n",
        "        # now that we have calculated the loss function, we can perform Gradient Descent\n",
        "        # we pass in the loss function and the weights that we want to update via Gradient Descent \n",
        "        grads = tape.gradient(total_loss, self.trainable_weights)\n",
        "        self.optimizer.apply_gradients(zip(grads, self.trainable_weights))\n",
        "        \n",
        "        # log the total loss\n",
        "        self.total_loss_tracker.update_state(total_loss)\n",
        "        \n",
        "        # log the reconsgrution loss \n",
        "        self.reconstruction_loss_tracker.update_state(reconstruction_loss)\n",
        "        \n",
        "        # log the kl loss \n",
        "        self.kl_loss_tracker.update_state(kl_loss)\n",
        "        \n",
        "        # return all the losses in a dictionary \n",
        "        return {\n",
        "            \"loss\": self.total_loss_tracker.result(),\n",
        "            \"reconstruction_loss\": self.reconstruction_loss_tracker.result(),\n",
        "            \"kl_loss\": self.kl_loss_tracker.result(),\n",
        "        }"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ez9_HmxpDh5g"
      },
      "source": [
        "## Loss Function \n",
        "\n",
        "Let's revist the VAE's loss function.\n",
        "\n",
        "From the video, and by reading the above code for the VAE class, we see that the Loss Function has two components . \n",
        "\n",
        "$$\\textbf{Total loss = binary crossentropy + KL Divergence}$$\n",
        "\n",
        "The first term we recognize as an old friend; recall we introduced **binary crossentropy** loss in Sprint 2 Module 3 as the loss function for binary classification problems. <br>\n",
        "\n",
        "\n",
        "The second term is the [**KL Divergence**](https://en.wikipedia.org/wiki/Kullback%E2%80%93Leibler_divergence) loss. \n",
        "\n",
        "**The KL divergence between a probability distribution and a reference distribution measures how much the probability distribution differs from the reference distribution.**\n",
        "\n",
        "Suppose we have a distribution of points but we don't actually know what kind of distribution it is. Is it a normal distribution, a binomial distribution, a Poisson distribution, or something else entirely? We don't know. \n",
        "\n",
        "So what we can do is form the hypothesis that the distribution of points is a normal distribution. How good is our hypothesis? Well, we can use the **KL Divergence** to measure how similar the distribution of points is to a normal distribution. The lower the **KL Divergence** score (i.e. the smaller the difference) between the two distributions, the better the match. \n",
        "\n",
        "Armed with that knowledge, let's return to our loss function and break it down. \n",
        "\n",
        "$$\\textbf{Total loss = binary crossentropy + KL Divergence}$$\n",
        "\n",
        "**binary crossentropy** is responsible for ensuring the quality of the model's predictions, i.e. (y_pred vs y_true). \n",
        "\n",
        "The **KL Divergence** term is responsible for ensuring that the latent probability distribution that our model learns is in fact a normal distribution. And to the extent that the latent probability distribution is not normal, the KL divergence score will differ from zero. If the learned latent probability distribution is nearly perfectly Normal, then the KL divergence will be close to zero. <br>\n",
        "\n",
        "Clearly, in order to accomplish both objectives, we want to drive both terms toward zero. \n",
        "\n",
        "To solve both these objectives at once, we create a new loss function by adding together the **binary crossentropy** and **KL Divergence** terms.\n",
        "\n",
        "### Further Readings\n",
        "\n",
        "- For a deeper explanation of **KL Divergence** using a simple and concrete example read [**Intuitive Guide to Understanding KL Divergence**](https://towardsdatascience.com/light-on-math-machine-learning-intuitive-guide-to-understanding-kl-divergence-2b382ca2b2a8). \n",
        "\n",
        "- For a deeper explanation of **KL Divergence** in the context of a VAE, read this [**Tutorial - What is a variational autoencoder?**](https://jaan.io/what-is-variational-autoencoder-vae-tutorial/) "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "_k6ELh7GDh5h"
      },
      "source": [
        "# load in mnist data set\n",
        "# we don't care about the y labels since this isn't a supervised classification task \n",
        "# this is an unsupervised variational auto-encoder \n",
        "(x_train, _), (x_test, _) = keras.datasets.mnist.load_data()\n",
        "\n",
        "# We don't care about the distinction between train and test sets here\n",
        "# so combine all data \n",
        "mnist_digits = np.concatenate([x_train, x_test], axis=0)\n",
        "\n",
        "# change shape and normalize pixel values between 0 and 1\n",
        "mnist_digits = np.expand_dims(mnist_digits, -1).astype(\"float32\") / 255.\n",
        "\n",
        "# instantiate a Variational Auto-Encoder model \n",
        "vae = VAE(encoder, decoder)\n",
        "\n",
        "# compile the model \n",
        "vae.compile(optimizer=keras.optimizers.Nadam())\n",
        "\n",
        "# train the model weights \n",
        "vae.fit(mnist_digits, epochs=30, batch_size=128, workers=10)\n",
        "# if you have access to multiple processors, set parameter `workers` to N - 1\n",
        "# where N is the total number of processors that you have "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zQN73eoEDh5h"
      },
      "source": [
        "### Visual Results \n",
        "\n",
        "Let's explore our modeling results visually in order to better understand them. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TnitunURDh5h"
      },
      "source": [
        "def generate_digits(digit_size, n, grid_x, grid_y, figure):\n",
        "    \n",
        "    \"\"\"\n",
        "    This function samples from the 2-Dimensional latent space Normal probability distribution \n",
        "    that our model as learned during training.\n",
        "    \"\"\"\n",
        "\n",
        "    ## This code samples images from the 2D Latent Space Normal Probability Distribtuion ##\n",
        "    # iterate through values in the y-axis\n",
        "    for i, yi in enumerate(grid_y):\n",
        "        # iterate through values in the x-axis\n",
        "        for j, xi in enumerate(grid_x):\n",
        "            # create a z vector using those (x,y) corrdinates \n",
        "            z_sample = np.array([[xi, yi]])\n",
        "            # pass in that z vector into th decoder model\n",
        "            # notice that we aren't passing in an image but rather a 2D coordinate, but a 2D coordinate of what?\n",
        "            # we are sampling from the latent space 2D Normal distribution\n",
        "            # we pass in a 2D coordinate, and the decoder generates an image from those coordinates\n",
        "            x_decoded = vae.decoder.predict(z_sample)\n",
        "            # reshape the image that the decoder returned \n",
        "            digit = x_decoded[0].reshape(digit_size, digit_size)\n",
        "            # store digit image in list for plotting\n",
        "            figure[\n",
        "                i * digit_size : (i + 1) * digit_size,\n",
        "                j * digit_size : (j + 1) * digit_size,\n",
        "            ] = digit\n",
        "                    \n",
        "    return figure\n",
        "\n",
        "\n",
        "def plot_latent_space(vae, n=30, figsize=15):\n",
        "    \"\"\"\n",
        "    This function plots the latent space Normal probability distribution that our model as learned during training. \n",
        "    Read through the comments to learn how. \n",
        "    \n",
        "    Parameters\n",
        "    ----------\n",
        "    vae: keras object\n",
        "        Trained Variational Auto-Encoder\n",
        "        \n",
        "    n: int\n",
        "        number of tick marks on the x and y axis \n",
        "    \"\"\"\n",
        "    \n",
        "    # display a n*n 2D manifold of  generated digits\n",
        "    digit_size = 28\n",
        "    scale = 1.0\n",
        "    figure = np.zeros((digit_size * n, digit_size * n))\n",
        "    \n",
        "    # linearly spaced coordinates corresponding to the 2D plot\n",
        "    # of digit classes in the latent space\n",
        "    # i.e. we are defining the tick marks of the plot\n",
        "    grid_x = np.linspace(-scale, scale, n)\n",
        "    grid_y = np.linspace(-scale, scale, n)[::-1]\n",
        "    \n",
        "    # use our trained VAE to generate digit images for us \n",
        "    figure = generate_digits(digit_size, n, grid_x, grid_y, figure)\n",
        "    \n",
        "    # plot all the digit images that we sampled \n",
        "    plt.figure(figsize=(figsize, figsize))\n",
        "    start_range = digit_size // 2\n",
        "    end_range = n * digit_size + start_range\n",
        "    pixel_range = np.arange(start_range, end_range, digit_size)\n",
        "    sample_range_x = np.round(grid_x, 1)\n",
        "    sample_range_y = np.round(grid_y, 1)\n",
        "    \n",
        "    \n",
        "    plt.xticks(pixel_range, sample_range_x)\n",
        "    plt.yticks(pixel_range, sample_range_y)\n",
        "    plt.xlabel(\"z[0]\")\n",
        "    plt.ylabel(\"z[1]\")\n",
        "    plt.imshow(figure, cmap=\"Greys_r\")\n",
        "    plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mb_Oy6OKDh5i"
      },
      "source": [
        "### Generative vs. Discriminative Models\n",
        "\n",
        "Whereas the models we have built up until today have been **Discriminative Models**, <br>Variational Autoencoders belong to the class of [**Generative Models**](https://towardsdatascience.com/generative-deep-learning-lets-seek-how-ai-extending-not-replacing-creative-process-fded15b0561b). \n",
        "\n",
        "Make sure to click on the link above to read about some really cool applications of generative models!\n",
        "\n",
        "Our VAE has learned a Latent Space 2-Dimensional Normal probability distribution that underlies the MNIST digit data set.\n",
        "\n",
        "The VAE's decoder can generate as many MNIST-like images as you want! We show an ensemble of MNIST-like images generated by our VAE in the plot below. Can you tell them apart from the real MNIST images?\n",
        "<br> \n",
        "\n",
        "Here are some examples of Discriminative and Generative Models:<br>\n",
        "**Discriminative models** learn the (hard or soft) boundary between classes. Examples are:\n",
        "\n",
        "    - Logistic Regression \n",
        "    - Random Forest\n",
        "    - SVM\n",
        "    - FCFF, LSTM, and CNN neural networks \n",
        "    \n",
        "**Generative models** learn the distribution of individual classes. Examples are:\n",
        "\n",
        "    - Naive Bayes classifier\n",
        "    - Variational Auto-Encoder \n",
        "    - Generative Adversarial Networks \n",
        "\n",
        "Another quite interesting kind of generative model is the Generative Adversarial Network, or GAN.<br>\n",
        "Here is a brief, readable introduction:\n",
        "\n",
        "#### [A Beginner's Guide to Generative Adversarial Networks (GANs)](https://wiki.pathmind.com/generative-adversarial-network-gan)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YzK2qJ6tQtUB"
      },
      "source": [
        "### Images generated from the latent space \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qq3prMB8Dh5i"
      },
      "source": [
        "plot_latent_space(vae)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "derLMcKtDh5i"
      },
      "source": [
        "### Plot Label Clusters\n",
        "\n",
        "The plot below shows the distribution of digit labels (represented by colors) in the latent space of the two components of the **z_mean** vector generated by the encoder. <br>\n",
        "\n",
        "The VAE passes the original MNIST digit images through the encoder and we plot the label as a function of the two components of the **z_mean** vector. <br>\n",
        "\n",
        "We can see, for example, that the model thinks that 0's and 1's are spatially dissimilar from 8's and 9's. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JC1zX3_wDh5j"
      },
      "source": [
        "def plot_label_clusters(vae, data, labels):\n",
        "    # display a 2D plot of the digit classes in the latent space\n",
        "    z_mean, _, _ = vae.encoder.predict(data)\n",
        "    plt.figure(figsize=(12, 10))\n",
        "    plt.scatter(z_mean[:, 0], z_mean[:, 1], c=labels)\n",
        "    plt.colorbar()\n",
        "    plt.xlabel(\"z[0]\")\n",
        "    plt.ylabel(\"z[1]\")\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "(x_train, y_train), _ = keras.datasets.mnist.load_data()\n",
        "x_train = np.expand_dims(x_train, -1).astype(\"float32\") / 255\n",
        "\n",
        "plot_label_clusters(vae, x_train, y_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hLmVYgLXDh5j"
      },
      "source": [
        "-----\n",
        "### Time for Questions\n",
        "\n",
        "In your own words, answer as many of the following questions as you can. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FJfTscf6Dh5j"
      },
      "source": [
        "**Question 1:**\n",
        "\n",
        "In a couple of simple sentences, **describe the differences between a Standard Auto-Encoder and a Variational Auto-Encoder.**\n",
        "\n",
        "Think how you would describe the differences to a non-data scientist over a cup of coffee. \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WWKre0u-Dh5k"
      },
      "source": [
        "**Answer 2:** A standard auto-encoder is to compress an object represented by huge numbers into a simplier object represented by less numbers selected from the huge numbers, and then learn features of the simplier object, try to regenerate a new object very close to the original object. A variational auto-encoder is to select distributions instead of numbers from the huge number to get a simplier object, and then do the same thing as auto-encoder."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bkm--dVEDh5k"
      },
      "source": [
        "**Question 2:**\n",
        "\n",
        "Now in technical detail, **describe the differences between a Standard Auto-Encoder and a Variational Auto-Encoder.** \n",
        "\n",
        "Think about how you would describe the differences in a technical interview. Imagine the interviewer hasn't asked you for in-depth technical details like the equation of the KL Divergence loss function, so you don't want to get lost in the weeds of technical detail. But you do want your explanation to show that you could talk about the technical details if he asked you a follow up question. \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rp-u7wP_Dh5k"
      },
      "source": [
        "**Answer 2:** A standard Auto-Encoder receives data, compresses it and then recreates the original input. A Variational Auto-Encoder assumes that the source data has some sort of underlying probability distribution (such as Gaussian) and then attempts to find the parameters of the distribution."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zXY3TH1iDh5l"
      },
      "source": [
        "**Question 3:**\n",
        "\n",
        "How would you describe the technical details of the **KL Divergence** loss function to a fellow data science student outside the context of a VAE?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rMg_abE9Dh5l"
      },
      "source": [
        "**Answer 3:** KL Divergence loss function describes difference between two distributions.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_6d3KwTADh5l"
      },
      "source": [
        "**Question 4:**\n",
        "\n",
        "How would you describe the technical details of the entire loss function **Binary CrossEntropy** and **KL Divergence** loss functions in the context of a VAE in a technical interview?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hly-Si55Dh5l"
      },
      "source": [
        "**Answer 4:** Binary crossentropy loss function compares each of the predicted probabilities to actual class output which is 0 or 1. Kullback-Leibler  Divergence loss function is a measure of how a probability distribution differs from another probability distribution."
      ]
    }
  ]
}